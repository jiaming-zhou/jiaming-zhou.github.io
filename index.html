<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>jiaming Zhou</title>

    <meta name="author" content="Jiaming Zhou">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/UST.svg.png" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Jiaming Zhou
                </p>
                <p>I am a third-year PhD student at <a href="https://www.hkust-gz.edu.cn">Hong Kong University of Science and Technology, Guangzhou</a>, under the supervision of <a href="https://junweiliang.me">Prof. Junwei Liang</a>.
                </p>
                <p>
                  Prior to this, I obtained my Bachelor's degree in Computer Science and Engineering, from Sichuan University in 2020. Then I received my Master degree in Computer Science and Engineering, at Sun Yat-Sen University in 2023, where I was advised by <a href="https://www.isee-ai.cn/~zhwshi/">Prof. Wei-Shi Zheng</a>.
                </p>
                <p>
                  I'm interested in computer vision and robotics. Currently, 
                  I mainly focus on <span style="color: #FF0066">generalizable robotic manipulation, embodied world model</span>.
                </p>

                <p>
                  <span style="color: #FF0066">I am looking for a research internship. If you have suitable opportunities, please feel free to contact me.</span>
                </p>
                <p style="text-align:center">
                  <a href="mailto:jia_ming_zhou@outlook.com">Email</a> &nbsp;/&nbsp;
                  <!-- <a href="data/JonBarron-CV.pdf">CV</a> &nbsp;/&nbsp; -->
                  <!-- <a href="data/JonBarron-bio.txt">Bio</a> &nbsp;/&nbsp; -->
                  <a href="https://scholar.google.com/citations?user=b3y40w8AAAAJ&hl=en">Scholar</a> &nbsp;/&nbsp;
                  <!-- <a href="https://twitter.com/jon_barron">Twitter</a> &nbsp;/&nbsp; -->
                  <a href="https://github.com/jiaming-zhou/">Github</a>
                </p>
              </td>
              <td style="padding:2.5%;width:23%;max-width:23%">
                <!-- <a href="images/photo1.jpg"><img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="images/photo1.jpg" class="hoverZoomLink"></a> -->
                <!-- <a href="images/photo1.jpg"><img style="width:100%;max-width:100%;object-fit: cover;" alt="profile photo" src="images/photo1.jpg" class="hoverZoomLink"></a> -->
                <a href="images/jiaming_hk.jpg"><img style="width:100%;max-width:100%;object-fit: cover;" alt="profile photo" src="images/jiaming_hk.jpg" class="hoverZoomLink"></a>
              </td>
            </tr>
          
          <!--
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Research</h2>
                <p>
                  I'm interested in computer vision and machine learning. Currently, I mainly focus on human action understanding and robot learning. 
                  Representative papers are <span class="highlight">highlighted</span>.
                </p>
              </td>
            </tr>


          -->


</tbody></table>
<table style="width:110%;border:0px;border-spacing:5px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
  <tr><h2>News</h2></tr>
  <tr> <td>&nbsp; &nbsp; &nbsp; &nbsp; <b>&#10053 09/2025:</b> One paper was accepted to NeurIPS 2025. <a href="https://jiaming-zhou.github.io/AGNOSTOS">[Generalizable Cross-task Manipulation]</a> </td></tr>
  <tr> <td>&nbsp; &nbsp; &nbsp; &nbsp; <b>&#10053 08/2025:</b> Two papers were accepted to CoRL 2025. <a href="https://teleema.github.io/projects/GLOVER++/">[GLOVER++]</a>, <a href="https://acodedog.github.io/OmniPerceptionPages/">[Omni-Perception]</a> </td></tr>
  <tr> <td>&nbsp; &nbsp; &nbsp; &nbsp; <b>&#10053 03/2025:</b> One paper was accepted to CVPR 2025. <a href="https://jiaming-zhou.github.io/projects/HumanRobotAlign/">[Human-Robot Alignment]</a> <a href="https://mp.weixin.qq.com/s/4D58UpJ7g3gmQbRyx0bCwQ">[量子位]</a> </td></tr>
  <tr> <td>&nbsp; &nbsp; &nbsp; &nbsp; <b>&#10053 09/2024:</b> One paper was accepted to CoRL 2024. <a href="https://teleema.github.io/projects/Sigma_Agent/">[Contrastive Imitation]</a> </td></tr>
  <tr> <td>&nbsp; &nbsp; &nbsp; &nbsp; <b>&#10053 07/2024:</b> One paper was accepted to TPAMI. <a href="https://arxiv.org/abs/2407.10860">[HCTransformer]</a> </td></tr>
  <tr> <td>&nbsp; &nbsp; &nbsp; &nbsp; <b>&#10053 03/2024:</b> Releasing the first cross-domain open-vocabulary action recognition benchmark! <a href="https://arxiv.org/abs/2403.01560">[XOV-Action]</a> </td></tr>
  <tr> <td>&nbsp; &nbsp; &nbsp; &nbsp; <b>&#10053 12/2023:</b> AdaptFocus for long-video action understanding was released. <a href="https://jiaming-zhou.github.io/projects/AdaFocus/">[AdaptFocus]</a> </td></tr>
  <tr> <td>&nbsp; &nbsp; &nbsp; &nbsp; <b>&#10053 09/2023:</b> One paper was accepted to NeurIPS 2023. <a href="https://arxiv.org/abs/2310.17942">[STDN]</a> </td></tr>
  <tr> <td>&nbsp; &nbsp; &nbsp; &nbsp; <b>&#10053 08/2023:</b> One paper was accepted to TMM 2023. <a href="https://ieeexplore.ieee.org/abstract/document/10210078">[TwinFormer]</a> </td></tr>
  <tr> <td>&nbsp; &nbsp; &nbsp; &nbsp; <b>&#10053 07/2022:</b> One paper was accepted to ECCV 2022. <a href="https://link.springer.com/chapter/10.1007/978-3-031-19827-4_31">[Cycle Inconsistency]</a> </td></tr>
  <tr> <td>&nbsp; &nbsp; &nbsp; &nbsp; <b>&#10053 03/2021:</b> One paper was accepted to CVPR 2021. <a href="https://openaccess.thecvf.com/content/CVPR2021/html/Zhou_Graph-Based_High-Order_Relation_Modeling_for_Long-Term_Action_Recognition_CVPR_2021_paper.html">[GHRM]</a> </td></tr>
  <tr>  <td> &nbsp; </td></tr>

</tbody></table>
<table style="width:110%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
  <tr>
      <h2>publications</h2> 
      <br>The purpose of my research is to <b>understand complex human activities and imitate them with real robots</b>. Currently, I focus on leveraging the <b>foundations in video and robotics domains</b> to develop <b>robotic generalists</b> in real-world scenarios.
      <br>
      <br>Representative papers are <span class="highlight">highlighted</span>.
  </tr>


<tr bgcolor="#dbf1f1">
  <td style="padding:20px;width:25%;vertical-align:middle">
    <img src="images/AGNOSTOS_arxiv.jpg" alt="PontTuset" width="180" style="border-style: none">
  </td>
  <td width="75%" valign="middle">
    <a href="https://arxiv.org/abs/2505.15660" id="AGNOSTOS_arxiv">
      <span class="papertitle">Exploring the Limits of Vision-Language-Action Manipulations in Cross-task Generalization</span>
    </a>
    <br>
    <strong>Jiaming Zhou</strong>, Ke Ye, Jiayi Liu, Teli Ma, Zifan Wang, Ronghe Qiu, Kun-Yu Lin, Zhilin Zhao, Junwei Liang
    <br>
    <em>NeurIPS, 2025</em>
    <br>
    <a href="https://arxiv.org/abs/2505.15660">[paper]</a>
    <a href="data/AGNOSTOS_arxiv_bib.html">[bibtex]</a>
    <a href="https://jiaming-zhou.github.io/AGNOSTOS">[project page]</a>
    <!-- <a href="https://mp.weixin.qq.com/s/4D58UpJ7g3gmQbRyx0bCwQ">[量子位]</a> -->
    <p style="font-size: 16px; margin-top: 2px; margin-bottom: 20px;">A cross-task manipulation generalization benchmark to evaluate existing Vision-Language-Action (VLA) models and a novel generalizable VLA method.</p>
  </td>
</tr>

<tr bgcolor="#dbf1f1">
  <td style="padding:20px;width:25%;vertical-align:middle">
    <img src="images/HRAlign_CVPR2025.jpg" alt="PontTuset" width="180" style="border-style: none">
  </td>
  <td width="75%" valign="middle">
    <a href="https://arxiv.org/abs/2406.14235" id="HRAlign_CVPR2025">
      <span class="papertitle">Mitigating the Human-Robot Domain Discrepancy in Visual Pre-training for Robotic Manipulation</span>
    </a>
    <br>
    <strong>Jiaming Zhou</strong>, Teli Ma, Kun-Yu Lin, Zifan Wang, Ronghe Qiu, Junwei Liang
    <br>
    <em>CVPR</em>, 2025
    <br>
    <a href="https://arxiv.org/abs/2406.14235">[paper]</a>
    <a href="data/HRAlign_arxiv2024_bib.html">[bibtex]</a>
    <a href="https://jiaming-zhou.github.io/projects/HumanRobotAlign/">[project page]</a>
    <a href="https://mp.weixin.qq.com/s/4D58UpJ7g3gmQbRyx0bCwQ">[量子位]</a>
    <p style="font-size: 16px; margin-top: 2px; margin-bottom: 20px;">A new paradigm utilizing paired human-robot videos to adapt human-data pretrained foundation models to robotic manipulation domain.</p>
  </td>
</tr>

<tr>
  <td style="padding:20px;width:25%;vertical-align:middle">
    <img src="images/supermimic_arxiv2025.jpg" alt="PontTuset" width="180" style="border-style: none">
  </td>
  <td width="75%" valign="middle">
    <a href="https://arxiv.org/abs/2509.22205" id="Supermimic_arxiv2025">
      <span class="papertitle">From Watch to Imagine Steering Long-horizon Manipulation via Human Demonstration and Future Envisionment</span>
    </a>
    <br>
    Ke Ye, <strong>Jiaming Zhou (co-first & project lead)</strong>, Yuanfeng Qiu, Jiayi Liu, Shihui Zhou, Kun-Yu Lin, Junwei Liang
    <br>
    <em>arxiv</em>, 2025
    <br>
    <a href="https://arxiv.org/abs/2509.22205">[paper]</a>
    <a href="data/supermimic_arxiv_bib.html">[bibtex]</a>
    <a href="https://yipko.com/super-mimic?utm_source=yks_blog&utm_medium=blog_post&utm_campaign=about_page">[project page]</a>
    <a href="https://mp.weixin.qq.com/s/hlUOXOzb-HG3d8SR3gQ3gQ?scene=1&click_id=3">[深蓝具身智能]</a>
    <p style="font-size: 16px; margin-top: 2px; margin-bottom: 20px;">A zero-shot long-horizon manipulation framework that mimics human long-range activities via demonstrations and achieves robust execution via generating visual future.</p>
  </td>
</tr>

<tr>
  <td style="padding:20px;width:25%;vertical-align:middle">
    <img src="images/Contrastive_IL_arXiv2024.jpg" alt="PontTuset" width="180" style="border-style: none">
  </td>
  <td width="75%" valign="middle">
    <a href="https://arxiv.org/abs/2406.09738" id="Contrastive_IL">
      <span class="papertitle">Contrastive Imitation Learning for Language-guided Multi-Task Robotic Manipulation</span>
    </a>
    <br>
    Teli Ma, <strong>Jiaming Zhou</strong>, Zifan Wang, Ronghe Qiu, Junwei Liang
    <br>
    <em>CoRL</em>, 2024
    <br>
    <a href="https://arxiv.org/pdf/2406.09738">[paper]</a>
    <a href="https://teleema.github.io/projects/Sigma_Agent/">[project page]</a>
    <a href="data/Contrastive_IL_arxiv2024_bib.html">[bibtex]</a> 
    <p></p>
  </td>
</tr>

<tr>
  <td style="padding:20px;width:25%;vertical-align:middle">
    <img src="images/GLOVER_arxiv2025.jpg" alt="PontTuset" width="180" style="border-style: none">
  </td>
  <td width="75%" valign="middle">
    <a href="https://arxiv.org/abs/2411.12286" id="GLOVER">
      <span class="papertitle">GLOVER: Generalizable Open-Vocabulary Affordance Reasoning
        for Task-Oriented Grasping</span>
    </a>
    <br>
    Teli Ma,  Zifan Wang, <strong>Jiaming Zhou</strong>, Mengmeng Wang, Junwei Liang
    <br>
    <em>arxiv</em>, 2025
    <br>
    <a href="https://arxiv.org/pdf/2411.12286">[paper]</a>
    <a href="https://teleema.github.io/projects/GLOVER/">[project page]</a>
    <a href="data/GLOVER_arxiv2025_bib.html">[bibtex]</a> 
    <p></p>
  </td>
</tr>

<tr>
  <td style="padding:20px;width:25%;vertical-align:middle">
    <img src="images/egotraj_arxiv2025.jpg" alt="PontTuset" width="180" style="border-style: none">
  </td>
  <td width="75%" valign="middle">
    <a href="https://arxiv.org/abs/2510.00405" id="EgoTraj-Bench">
      <span class="papertitle">EgoTraj-Bench: Towards Robust Trajectory Prediction Under Ego-view Noisy Observations</span>
    </a>
    <br>
    Jiayi Liu, <strong>Jiaming Zhou</strong>, Ke Ye, Kun-Yu Lin, Allan Wang, Junwei Liang
    <br>
    <em>arxiv</em>, 2025
    <br>
    <a href="https://arxiv.org/abs/2510.00405">[paper]</a>
    <a href="data/egotraj_arxiv_bib.html">[bibtex]</a> 
    <p></p>
  </td>
</tr>

<tr>
  <td style="padding:20px;width:25%;vertical-align:middle">
    <img src="images/OmniPerception.png" alt="OmniPerception" width="180" style="border-style: none">
  </td>
  <td width="75%" valign="middle">
    <a href="https://acodedog.github.io/OmniPerceptionPages/" id="OmniPerception">
      <span class="papertitle">Omni-Perception: Omnidirectional Collision Avoidance for Legged Locomotion in Dynamic Environments</span>
    </a>
    <br>
    Zifan Wang, Teli Ma, Yufei Jia, Xun Yang, <strong>Jiaming Zhou</strong>, Wenlong Ouyang, Qiang Zhang, Junwei Liang
    <br>
    <em>CoRL</em>, 2025
    <br>
    <a href="https://arxiv.org/abs/2505.19214">[paper]</a>
    <a href="https://acodedog.github.io/OmniPerceptionPages/">[project page]</a>
    <a href="data/OmniPerception_arxiv2025_bib.html">[bibtex]</a>
    <p></p>
  </td>
</tr>


<tr>
  <td style="padding:20px;width:25%;vertical-align:middle">
    <img src="images/glover++.jpg" alt="GLOVER++" width="180" style="border-style: none">
  </td>
  <td width="75%" valign="middle">
    <a href="https://teleema.github.io/projects/GLOVER++/" id="GLOVER++">
      <span class="papertitle">GLOVER++: Enhanced Generalizable Open-Vocabulary Affordance Reasoning for Task-Oriented Grasping</span>
    </a>
    <br>
    Teli Ma, Jia Zheng, Zifan Wang, Ziyao Gao, <strong>Jiaming Zhou</strong>, Junwei Liang
    <br>
    <em>CoRL</em>, 2025
    <br>
    <a href="https://arxiv.org/pdf/2505.11865">[paper]</a>
    <a href="https://teleema.github.io/projects/GLOVER++/">[project page]</a>
    <a href="data/GLOVER++_arxiv2025_bib.html">[bibtex]</a>
    <p></p>
  </td>
</tr>




<tr>
  <td style="padding:20px;width:25%;vertical-align:middle">
    <img src="images/AdaFocus_arXiv2023.jpg" alt="PontTuset" width="180" style="border-style: none">
  </td>
  <td width="75%" valign="middle">
    <a href="https://arxiv.org/pdf/2311.17118.pdf" id="AdaFocus_arxiv2023">
      <span class="papertitle">AdaFocus: Towards End-to-end Weakly Supervised Learning for Long-Video Action Understanding</span>
    </a>
    <br>
    <strong>Jiaming Zhou</strong>, Hanjun Li, Kun-Yu Lin, Junwei Liang
    <br>
    <em>arxiv</em>, 2024
    <br>
    <a href="https://arxiv.org/abs/2311.17118">[paper]</a>
    <!-- <a href="https://jiaming-zhou.github.io/">supp</a> / -->
    <a href="data/AdaFocus_arxiv2023_bib.html">[bibtex]</a>
    <a href="https://jiaming-zhou.github.io/projects/AdaFocus/">[project page]</a>
    <p style="font-size: 16px; margin-top: 2px; margin-bottom: 20px;">The first weakly supervised framework for developing efficient end-to-end action recognition models on long videos, which gives birth to a new weakly supervised pipeline for downstream long-video tasks.</p>
  </td>
</tr>



<tr>
  <td style="padding:20px;width:25%;vertical-align:middle">
    <img src="images/ActionHub_arXiv2024.jpg" alt="PontTuset" width="180" style="border-style: none">
  </td>
  <td width="75%" valign="middle">
    <a href="https://arxiv.org/pdf/2401.11654.pdf" id="ActionHub_arxiv2024">
      <span class="papertitle">ActionHub: A Large-scale Action Video Description Dataset for Zero-shot Action Recognition</span>
    </a>
    <br>
    <strong>Jiaming Zhou</strong>, Junwei Liang, Kun-Yu Lin, Jinrui Yang, Wei-Shi Zheng
    <br>
    <em>arxiv</em>, 2024
    <br>
    <a href="https://arxiv.org/abs/2401.11654">[paper]</a>
    <!-- <a href="https://jiaming-zhou.github.io/">supp</a> / -->
    <a href="data/ActionHub_arxiv2024_bib.html">[bibtex]</a>
    <!-- <a href="https://jiaming-zhou.github.io/projects/AdaFocus/">[project page]</a> -->
    <p style="font-size: 16px; margin-top: 2px; margin-bottom: 20px;">A large-scale action video description dataset named ActionHub is proposed, which is the first, and the largest dataset that provides millions of video descriptions to describe thousands of human actions.</p>

  </td>
</tr>

<tr>
  <td style="padding:20px;width:25%;vertical-align:middle">
    <img src="images/TwinFormer_TMM2023.jpg" alt="PontTuset" width="180" style="border-style: none">
  </td>
  <td width="75%" valign="middle">
    <a href="https://ieeexplore.ieee.org/abstract/document/10210078" id="TwinFormer_TMM2023">
      <span class="papertitle">TwinFormer: Fine-to-Coarse Temporal Modeling for Long-term Action Recognition</span>
    </a>
    <br>
    <strong>Jiaming Zhou</strong>, Kun-Yu Lin, Yu-Kun Qiu, Wei-Shi Zheng
    <br>
    <em>Transactions on Multimedia (TMM)</em>, 2023
    <br>
    <a href="https://ieeexplore.ieee.org/abstract/document/10210078">[paper]</a>
    <a href="data/TwinFormer_TMM2023_bib.html">[bibtex]</a> 
    <p></p>
  </td>
</tr>

<tr>
  <td style="padding:20px;width:25%;vertical-align:middle">
    <img src="images/GHRM_CVPR2021.jpg" alt="PontTuset" width="180" style="border-style: none">
  </td>
  <td width="75%" valign="middle">
    <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Zhou_Graph-Based_High-Order_Relation_Modeling_for_Long-Term_Action_Recognition_CVPR_2021_paper.pdf" id="GHRM_CVPR2021">
      <span class="papertitle">Graph-based High-order Relation Modeling for Long-term Action Recognition</span>
    </a>
    <br>
    <strong>Jiaming Zhou</strong>, Kun-Yu Lin, Haoxin Li, Wei-Shi Zheng
    <br>
    <em>CVPR</em>, 2021
    <br>
    <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Zhou_Graph-Based_High-Order_Relation_Modeling_for_Long-Term_Action_Recognition_CVPR_2021_paper.pdf">[paper]</a>
    <a href="https://openaccess.thecvf.com/content/CVPR2021/supplemental/Zhou_Graph-Based_High-Order_Relation_CVPR_2021_supplemental.pdf">[supp]</a>
    <a href="data/GHRM_CVPR2021_bib.html">[bibtex]</a> 
    <p></p>
    <!-- <p>We produce state-of-the-art contours, regions and object candidates, and we compute normalized-cuts eigenvectors 20&times faster.</p> -->
    <!-- <p>This paper subsumes our CVPR 2014 paper.</p> -->
  </td>
</tr>


<tr>
  <td style="padding:20px;width:25%;vertical-align:middle">
    <img src="images/Cycle_ECCV2022.jpg" alt="PontTuset" width="180" style="border-style: none">
  </td>
  <td width="75%" valign="middle">
    <a href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136930520.pdf" id="Cycle_ECCV2022">
      <span class="papertitle">Adversarial Partial Domain Adaptation by Cycle Inconsistency</span>
    </a>
    <br>
    Kun-Yu Lin, <strong>Jiaming Zhou</strong> (co-first), Yu-Kun Qiu, Wei-Shi Zheng
    <br>
    <em>ECCV</em>, 2022
    <br>
    <a href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136930520.pdf">[paper]</a>
    <a href="https://static-content.springer.com/esm/chp%3A10.1007%2F978-3-031-19827-4_31/MediaObjects/540002_1_En_31_MOESM1_ESM.pdf">[supp]</a>
    <a href="data/Cycle_inconsistency_ECCV2022_bib.html">[bibtex]</a> 
    <p></p>
  </td>
</tr>

<tr>
  <td style="padding:20px;width:25%;vertical-align:middle">
    <img src="images/HCTransformer_TPAMI.png" alt="PontTuset" width="180" style="border-style: none">
  </td>
  <td width="75%" valign="middle">
    <a href="https://arxiv.org/abs/2407.10860" id="HCTransformer">
      <span class="papertitle">Human-Centric Transformer for Domain Adaptive Action Recognition</span>
    </a>
    <br>
    Kun-Yu Lin, <strong>Jiaming Zhou</strong>, Wei-Shi Zheng
    <br>
    <em>TPAMI</em>, 2024
    <br>
    <a href="https://arxiv.org/pdf/2407.10860">[paper]</a>
    <!--<a href="data/Contrastive_IL_arxiv2024_bib.html">[bibtex]</a>-->
    <p></p>
  </td>
</tr>


<tr>
  <td style="padding:20px;width:25%;vertical-align:middle">
    <img src="images/XOV_Action_arXiv2024.jpg" alt="PontTuset" width="180" style="border-style: none">
  </td>
  <td width="75%" valign="middle">
    <a href="https://arxiv.org/pdf/2403.01560.pdf" id="XOV_Action_arxiv2024">
      <span class="papertitle">Rethinking CLIP-based Video Learners in Cross-Domain Open-Vocabulary Action Recognition</span>
    </a>
    <br>
    Kun-Yu Lin, Henghui Ding, <strong>Jiaming Zhou</strong>, Yi-Xing Peng, Zhilin Zhao, Chen Change Loy, Wei-Shi Zheng
    <br>
    <em>arxiv</em>, 2024
    <br>
    <a href="https://arxiv.org/abs/2403.01560">[paper]</a>
    <!-- <a href="https://jiaming-zhou.github.io/">supp</a> / -->
    <a href="data/XOV_Action_arxiv2024_bib.html">[bibtex]</a>
    <a href="https://github.com/KunyuLin/XOV-Action/">[github]</a>
    <p style="font-size: 16px; margin-top: 2px; margin-bottom: 20px;">The first benchmark, named XOV-Action, for the cross-domain open-vocabulary action recognition task, and a simple yet effective method to address the scene bias for the task.</p>
  </td>
</tr>


<tr>
  <td style="padding:20px;width:25%;vertical-align:middle">
    <img src="images/GeoDeformer_arXiv2023.jpg" alt="PontTuset" width="180" style="border-style: none">
  </td>
  <td width="75%" valign="middle">
    <a href="https://arxiv.org/pdf/2311.17975.pdf" id="GeoDeformer_arXiv2023">
      <span class="papertitle">GeoDeformer: Geometric Deformable Transformer for Action Recognition</span>
    </a>
    <br>
    Jinhui Ye, <strong>Jiaming Zhou</strong>, Hui Xiong, Junwei Liang
    <br>
    <em>arxiv</em>, 2023
    <br>
    <a href="https://arxiv.org/pdf/2311.17975.pdf">[paper]</a>
    <a href="data/GeoDeformer_arxiv2023_bib.html">[bibtex]</a> 
    <p></p>
  </td>
</tr>

<tr>
  <td style="padding:20px;width:25%;vertical-align:middle">
    <img src="images/STDN_NeurIPS2023.jpg" alt="PontTuset" width="180" style="border-style: none">
  </td>
  <td width="75%" valign="middle">
    <a href="https://openreview.net/forum?id=YsZTDcIQwQ" id="STDN_NeurIPS2023">
      <span class="papertitle">Diversifying Spatial-Temporal Perception for Video Domain Generalization</span>
    </a>
    <br>
    Kun-Yu Lin, Jia-Run Du, Yipeng Gao, <strong>Jiaming Zhou</strong>, Wei-Shi Zheng
    <br>
    <em>NeurIPS</em>, 2023
    <br>
    <a href="https://openreview.net/pdf?id=YsZTDcIQwQ">[paper]</a>
    <a href="https://openreview.net/attachment?id=YsZTDcIQwQ&name=supplementary_material">[supp]</a>
    <a href="data/STDN_NeurIPS2023_bib.html">[bibtex]</a> 
    <a href="https://github.com/KunyuLin/STDN/">[github]</a>
    <p></p>
  </td>
</tr>





<tr>  <td> &nbsp; </td></tr>



</tbody></table>
<table style="width:110%;border:0px;border-spacing:5px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
  <tr><h2>Services</h2></tr>
  <tr> <td>&nbsp; &nbsp; &nbsp; &nbsp; Reviewers of <b>CVPR</b> 2022-Now, <b>ECCV</b> 2022-Now, <b>ICCV</b> 2023-Now. </td></tr>
  <tr> <td>&nbsp; &nbsp; &nbsp; &nbsp; Reviewers of <b>NeurIPS</b> 2024-Now, <b>ICML</b> 2024-Now, <b>ICLR</b> 2024-Now. </td></tr>
  <tr> <td>&nbsp; &nbsp; &nbsp; &nbsp; Reviewer of <b>Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</b>.</td></tr>
  <tr> <td>&nbsp; &nbsp; &nbsp; &nbsp; Reviewer of <b>Transactions on Multimedia</b>.</td></tr>
  <tr> <td>&nbsp; &nbsp; &nbsp; &nbsp; Reviewer of <b>Pattern Recognition</b>.</td></tr>
  <tr>  <td> &nbsp; </td></tr>


<!--
</tbody></table>
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
  <tr>
    <td>
      <h2>Miscellanea</h2>
    </td>
  </tr>
</tbody></table>
<table width="100%" align="center" border="0" cellpadding="20"><tbody>
  
  <tr>
    <td style="padding:20px;width:25%;vertical-align:middle"><img src="images/cvf.jpg"></td>
    <td width="75%" valign="center">
      <a href="https://cvpr2023.thecvf.com/Conferences/2023/Organizers">Demo Chair, CVPR 2023</a>
<br>
      <a href="https://cvpr2022.thecvf.com/area-chairs">Area Chair, CVPR 2022</a>
      <br>
      <a href="http://cvpr2021.thecvf.com/area-chairs">Area Chair & Longuet-Higgins Award Committee Member, CVPR 2021</a>
      <br>
      <a href="http://cvpr2019.thecvf.com/area_chairs">Area Chair, CVPR 2019</a>
      <br>
      <a href="http://cvpr2018.thecvf.com/organizers/area_chairs">Area Chair, CVPR 2018</a>
    </td>
  </tr>
  <tr>
    <td style="padding:20px;width:25%;vertical-align:middle">
      <img src="images/cs188.jpg" alt="cs188">
    </td>
    <td width="75%" valign="center">
      <a href="http://inst.eecs.berkeley.edu/~cs188/sp11/announcements.html">Graduate Student Instructor, CS188 Spring 2011</a>
      <br>
      <a href="http://inst.eecs.berkeley.edu/~cs188/fa10/announcements.html">Graduate Student Instructor, CS188 Fall 2010</a>
      <br>
      <a href="http://aima.cs.berkeley.edu/">Figures, "Artificial Intelligence: A Modern Approach", 3rd Edition</a>
    </td>
  </tr>

  <tr>
    <td align="center" style="padding:20px;width:25%;vertical-align:middle">
      <h2>Basically <br> Blog Posts</h2>
    </td>
    <td width="75%" valign="middle">
      <a href="https://arxiv.org/abs/2112.11687">Squareplus: A Softplus-Like Algebraic Rectifier</a>
      <br>
      <a href="https://arxiv.org/abs/2010.09714">A Convenient Generalization of Schlick's Bias and Gain Functions</a>
      <br>
      <a href="https://arxiv.org/abs/1704.07483">Continuously Differentiable Exponential Linear Units</a>
      <br>
      <a href="https://jonbarron.info/data/cvpr2023_llm_workshop_annotated.pdf">Scholars & Big Models: How Can Academics Adapt?</a>
    </td>
  </tr>
  
  
</tbody></table>
-->

<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
  <tr>
    <td style="padding:0px">
      <br>
      <p style="text-align:center;font-size:small;">
        This website borrows from <a href="https://github.com/jonbarron/jonbarron_website">Jon Barron</a>.
      </p>
    </td>
  </tr>
</tbody></table>
        
        </td>
      </tr>
    </table>
  </body>
</html>
