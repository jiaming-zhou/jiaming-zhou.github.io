<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>jiaming Zhou</title>

    <meta name="author" content="Jiaming Zhou">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/UST.svg.png" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Jiaming Zhou
                </p>
                <p>I am a second-year PhD student at <a href="https://www.hkust-gz.edu.cn">Hong Kong University of Science and Technology, Guangzhou</a>, under the supervision of <a href="https://junweiliang.me">Prof. Junwei Liang</a>.
                </p>
                <p>
                  Prior to this, I obtained my Bachelor's degree in Computer Science and Engineering, from Sichuan University in 2020. Then I received my Master degree in Computer Science and Engineering, at Sun Yat-Sen University in 2023, where I was advised by <a href="https://www.isee-ai.cn/~zhwshi/">Prof. Wei-Shi Zheng</a>.
                </p>
                <p>
                  I'm interested in computer vision and robotics. Currently, I mainly focus on <span style="color: #FF0066">human action understanding and robot learning</span>.
                </p>
                <p style="text-align:center">
                  <a href="mailto:jia_ming_zhou@outlook.com">Email</a> &nbsp;/&nbsp;
                  <!-- <a href="data/JonBarron-CV.pdf">CV</a> &nbsp;/&nbsp; -->
                  <!-- <a href="data/JonBarron-bio.txt">Bio</a> &nbsp;/&nbsp; -->
                  <a href="https://scholar.google.com/citations?user=b3y40w8AAAAJ&hl=en">Scholar</a> &nbsp;/&nbsp;
                  <!-- <a href="https://twitter.com/jon_barron">Twitter</a> &nbsp;/&nbsp; -->
                  <a href="https://github.com/jiaming-zhou/">Github</a>
                </p>
              </td>
              <td style="padding:2.5%;width:23%;max-width:23%">
                <!-- <a href="images/photo1.jpg"><img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="images/photo1.jpg" class="hoverZoomLink"></a> -->
                <!-- <a href="images/photo1.jpg"><img style="width:100%;max-width:100%;object-fit: cover;" alt="profile photo" src="images/photo1.jpg" class="hoverZoomLink"></a> -->
                <a href="images/jiaming_hk.jpg"><img style="width:100%;max-width:100%;object-fit: cover;" alt="profile photo" src="images/jiaming_hk.jpg" class="hoverZoomLink"></a>
              </td>
            </tr>
          
          <!--
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Research</h2>
                <p>
                  I'm interested in computer vision and machine learning. Currently, I mainly focus on human action understanding and robot learning. 
                  Representative papers are <span class="highlight">highlighted</span>.
                </p>
              </td>
            </tr>


          -->


</tbody></table>
<table style="width:110%;border:0px;border-spacing:5px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
  <tr><h2>News</h2></tr>
  <tr> <td>&nbsp; &nbsp; &nbsp; &nbsp; <b>&#10053 03/2025:</b> One paper was accepted to CVPR 2025. <a href="https://jiaming-zhou.github.io/projects/HumanRobotAlign/">[Human-Robot Alignment]</a> <a href="https://mp.weixin.qq.com/s/4D58UpJ7g3gmQbRyx0bCwQ">[量子位]</a> </td></tr>
  <tr> <td>&nbsp; &nbsp; &nbsp; &nbsp; <b>&#10053 09/2024:</b> One paper was accepted to CoRL 2024. <a href="https://teleema.github.io/projects/Sigma_Agent/">[Contrastive Imitation]</a> </td></tr>
  <tr> <td>&nbsp; &nbsp; &nbsp; &nbsp; <b>&#10053 07/2024:</b> One paper was accepted to TPAMI. <a href="https://arxiv.org/abs/2407.10860">[HCTransformer]</a> </td></tr>
  <tr> <td>&nbsp; &nbsp; &nbsp; &nbsp; <b>&#10053 03/2024:</b> Releasing the first cross-domain open-vocabulary action recognition benchmark! <a href="https://arxiv.org/abs/2403.01560">[XOV-Action]</a> </td></tr>
  <tr> <td>&nbsp; &nbsp; &nbsp; &nbsp; <b>&#10053 12/2023:</b> AdaptFocus for long-video action understanding was released. <a href="https://jiaming-zhou.github.io/projects/AdaFocus/">[AdaptFocus]</a> </td></tr>
  <tr> <td>&nbsp; &nbsp; &nbsp; &nbsp; <b>&#10053 09/2023:</b> One paper was accepted to NeurIPS 2023. <a href="https://arxiv.org/abs/2310.17942">[STDN]</a> </td></tr>
  <tr> <td>&nbsp; &nbsp; &nbsp; &nbsp; <b>&#10053 08/2023:</b> One paper was accepted to TMM 2023. <a href="https://ieeexplore.ieee.org/abstract/document/10210078">[TwinFormer]</a> </td></tr>
  <tr> <td>&nbsp; &nbsp; &nbsp; &nbsp; <b>&#10053 07/2022:</b> One paper was accepted to ECCV 2022. <a href="https://link.springer.com/chapter/10.1007/978-3-031-19827-4_31">[Cycle Inconsistency]</a> </td></tr>
  <tr> <td>&nbsp; &nbsp; &nbsp; &nbsp; <b>&#10053 03/2021:</b> One paper was accepted to CVPR 2021. <a href="https://openaccess.thecvf.com/content/CVPR2021/html/Zhou_Graph-Based_High-Order_Relation_Modeling_for_Long-Term_Action_Recognition_CVPR_2021_paper.html">[GHRM]</a> </td></tr>
  <tr>  <td> &nbsp; </td></tr>

</tbody></table>
<table style="width:110%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
  <tr>
      <h2>publications</h2> 
      <br>The purpose of my research is to understand complex human activities and imitate them with real robots. Currently, I focus on leveraging the foundations in video and robotics domains to develop robotic generalists in real-world scenarios.
      <br>
      <br>Representative papers are <span class="highlight">highlighted</span>.
  </tr>
<!-- <tr onmouseout="difsurvey_stop()" onmouseover="difsurvey_start()">
  <td style="padding:20px;width:25%;vertical-align:middle">
    <div class="one">
    <div class="two" id='difsurvey_image'><video  width=100% height=100% muted autoplay loop>
    <source src="images/difsurvey_video.mp4" type="video/mp4">
    Your browser does not support the video tag.
    </video></div>
      <img src='images/difsurvey_image.jpg' width="160">
    </div>
    <script type="text/javascript">
      function difsurvey_start() {
        document.getElementById('difsurvey_image').style.opacity = "1";
      }

      function difsurvey_stop() {
        document.getElementById('difsurvey_image').style.opacity = "0";
      }
      difsurvey_stop()
    </script>
  </td>
  <td style="padding:20px;width:75%;vertical-align:middle">
    <a href="https://arxiv.org/abs/2310.07204">
      <span class="papertitle">State of the Art on Diffusion Models for Visual Computing
</span>
    </a>
    <br>
	<a href="https://ryanpo.com/">Ryan Po</a>,
	<a href="https://yifita.netlify.app/">Wang Yifan</a>,
	<a href="https://people.mpi-inf.mpg.de/~golyanik/">Vladislav Golyanik</a>,
	<a href="https://kfiraberman.github.io/">Kfir Aberman</a>,
	<strong>Jonathan T. Barron</strong>,
	<a href="https://www.cs.tau.ac.il/~amberman/">Amit H. Bermano</a>,
	<a href="https://ericryanchan.github.io/">Eric Ryan Chan</a>,
	<a href="https://www.weizmann.ac.il/math/dekel/home">Tali Dekel</a>,
	<a href="https://holynski.org/">Aleksander Holynski</a>,
	<a href="https://people.eecs.berkeley.edu/~kanazawa/">Angjoo Kanazawa</a>,
	<a href="https://tml.stanford.edu/">C. Karen Liu</a>,
	<a href="https://lingjie0206.github.io/">Lingjie Liu</a>,
	<a href="https://bmild.github.io/">Ben Mildenhall</a>,
    <a href="https://www.niessnerlab.org/">Matthias Nießner</a>,
	<a href="https://ommer-lab.com/people/ommer/">Björn Ommer</a>,
	<a href="https://people.mpi-inf.mpg.de/~theobalt/">Christian Theobalt</a>,
	<a href="https://peterwonka.net/">Peter Wonka</a>,
    <a href="https://stanford.edu/~gordonwz/">Gordon Wetzstein</a>
    <br>
	<em>arXiv<em>, 2023
    <br>
    <p></p>
    <p>
    A survey of recent progress in diffusion models for images, videos, and 3D.
    </p>
  </td>
</tr>           -->

<!-- <tr onmouseout="mipnerf_stop()" onmouseover="mipnerf_start()"  bgcolor="#ffffd0">
  <td style="padding:20px;width:25%;vertical-align:middle">
    <div class="one">
      <div class="two" id='mipnerf_image'><video  width=100% height=100% muted autoplay loop>
      <source src="images/mipnerf_ipe_yellow.mp4" type="video/mp4">
      Your browser does not support the video tag.
      </video></div>
      <img src='images/mipnerf_ipe_yellow.png' width="160">
    </div>
    <script type="text/javascript">
      function mipnerf_start() {
        document.getElementById('mipnerf_image').style.opacity = "1";
      }
      function mipnerf_stop() {
        document.getElementById('mipnerf_image').style.opacity = "0";
      }
      mipnerf_stop()
    </script>
  </td>
  <td style="padding:20px;width:75%;vertical-align:middle">
    <a href="http://jonbarron.info/mipnerf">
      <span class="papertitle">Mip-NeRF: A Multiscale Representation for Anti-Aliasing Neural Radiance Fields</span>
    </a>
    <br>
    <strong>Jonathan T. Barron</strong>,
    <a href="https://bmild.github.io/">Ben Mildenhall</a>,
    <a href="http://matthewtancik.com/">Matthew Tancik</a>, <br>
    <a href="https://phogzone.com/">Peter Hedman</a>,
    <a href="http://www.ricardomartinbrualla.com/">Ricardo Martin-Brualla</a>,
    <a href="https://pratulsrinivasan.github.io/">Pratul Srinivasan</a>
    <br>
    <em>ICCV</em>, 2021 &nbsp <font color="red"><strong>(Oral Presentation, Best Paper Honorable Mention)</strong></font>
    <br>
    <a href="http://jonbarron.info/mipnerf">project page</a>
    /
    <a href="https://arxiv.org/abs/2103.13415">arXiv</a>
    /
    <a href="https://youtu.be/EpH175PY1A0">video</a>
    /
    <a href="https://github.com/google/mipnerf">code</a>
    <p></p>
    <p>NeRF is aliased, but we can anti-alias it by casting cones and prefiltering the positional encoding function.</p>
  </td>
</tr>  -->

<!-- <tr>
  <td style="padding:20px;width:25%;vertical-align:middle">
    <img src="images/PABMM2015.jpg" alt="PontTuset" width="160" style="border-style: none">
  </td>
  <td width="75%" valign="middle">
    <a href="https://arxiv.org/abs/1503.00848" id="MCG_journal">
      <span class="papertitle">Multiscale Combinatorial Grouping for Image Segmentation and Object Proposal Generation</span>
    </a>
    <br>
    <a href="http://imatge.upc.edu/web/people/jordi-pont-tuset">Jordi Pont-Tuset</a>, <a href="http://www.cs.berkeley.edu/~arbelaez/">Pablo Arbel&aacuteez</a>, <strong>Jonathan T. Barron</strong>, <a href="http://imatge.upc.edu/web/ferran">Ferran Marqu&eacutes</a>, <a href="http://www.cs.berkeley.edu/~malik/">Jitendra Malik</a>
    <br>
    <em>TPAMI</em>, 2017
    <br>
    <a href="http://www.eecs.berkeley.edu/Research/Projects/CS/vision/grouping/mcg/">project page</a> /
    <a href="data/PontTusetTPAMI2017.bib">bibtex</a> /
    <a href="https://drive.google.com/file/d/1AiB78Fy7QVA3KqgcooyzMAC5L8HhNzjz/view?usp=sharing">fast eigenvector code</a>
    <p></p>
    <p>We produce state-of-the-art contours, regions and object candidates, and we compute normalized-cuts eigenvectors 20&times faster.</p>
    <p>This paper subsumes our CVPR 2014 paper.</p>
  </td>
</tr> -->

<tr bgcolor="#dbf1f1">
  <td style="padding:20px;width:25%;vertical-align:middle">
    <img src="images/AGNOSTOS_arxiv.jpg" alt="PontTuset" width="180" style="border-style: none">
  </td>
  <td width="75%" valign="middle">
    <!-- <a href="https://arxiv.org/abs/2406.14235" id="HRAlign_CVPR2025"> -->
      <span class="papertitle">Exploring the Limits of Vision-Language-Action Manipulations in Cross-task Generalization</span>
    </a>
    <br>
    <strong>Jiaming Zhou</strong>, Ke Ye, Jiayi LIU, Teli Ma, Zifan Wang, Ronghe Qiu, Kun-Yu Lin, Zhilin Zhao, Junwei Liang
    <br>
    <em>to be summited to top conference</em>
    <br>
    <!-- <a href="https://arxiv.org/abs/2406.14235">[paper]</a> -->
    <!-- <a href="https://jiaming-zhou.github.io/">supp</a> / -->
    <!-- <a href="data/HRAlign_arxiv2024_bib.html">[bibtex]</a> -->
    <!-- <a href="https://jiaming-zhou.github.io/projects/HumanRobotAlign/">[project page]</a> -->
    <!-- <a href="https://mp.weixin.qq.com/s/4D58UpJ7g3gmQbRyx0bCwQ">[量子位]</a> -->
    <p style="font-size: 16px; margin-top: 2px; margin-bottom: 20px;">Proposing a cross-task manipulation generalization benchmark to evaluate existing Vision-Language-Action (VLA) models and a novel generalizable VLA method.</p>
  </td>
</tr>

<tr bgcolor="#dbf1f1">
  <td style="padding:20px;width:25%;vertical-align:middle">
    <img src="images/HRAlign_CVPR2025.jpg" alt="PontTuset" width="180" style="border-style: none">
  </td>
  <td width="75%" valign="middle">
    <a href="https://arxiv.org/abs/2406.14235" id="HRAlign_CVPR2025">
      <span class="papertitle">Mitigating the Human-Robot Domain Discrepancy in Visual Pre-training for Robotic Manipulation</span>
    </a>
    <br>
    <strong>Jiaming Zhou</strong>, <a href="https://scholar.google.com/citations?user=arny77IAAAAJ&hl=en">Teli Ma</a>, <a href="https://scholar.google.com/citations?user=tkUBeeQAAAAJ&hl=en">Kun-Yu Lin</a>, <a href="https://openreview.net/profile?id=~Zifan_Wang7">Zifan Wang</a>, <a href="https://scholar.google.com/citations?hl=en&user=QOwOHZMAAAAJ">Ronghe Qiu</a>, <a href="https://scholar.google.com/citations?hl=en&user=bMedjfUAAAAJ">Junwei Liang</a>
    <br>
    <em>CVPR</em>, 2025
    <br>
    <a href="https://arxiv.org/abs/2406.14235">[paper]</a>
    <!-- <a href="https://jiaming-zhou.github.io/">supp</a> / -->
    <a href="data/HRAlign_arxiv2024_bib.html">[bibtex]</a>
    <a href="https://jiaming-zhou.github.io/projects/HumanRobotAlign/">[project page]</a>
    <a href="https://mp.weixin.qq.com/s/4D58UpJ7g3gmQbRyx0bCwQ">[量子位]</a>
    <p style="font-size: 16px; margin-top: 2px; margin-bottom: 20px;">A new paradigm utilizing paired human-robot videos to adapt human-data pretrained foundation models to robotic manipulation domain.</p>
  </td>
</tr>


<tr bgcolor="#dbf1f1">
  <td style="padding:20px;width:25%;vertical-align:middle">
    <img src="images/AdaFocus_arXiv2023.jpg" alt="PontTuset" width="180" style="border-style: none">
  </td>
  <td width="75%" valign="middle">
    <a href="https://arxiv.org/pdf/2311.17118.pdf" id="AdaFocus_arxiv2023">
      <span class="papertitle">AdaFocus: Towards End-to-end Weakly Supervised Learning for Long-Video Action Understanding</span>
    </a>
    <br>
    <strong>Jiaming Zhou</strong>, <a href="https://scholar.google.com/citations?user=nEjwfNEAAAAJ&hl=zh-CN">Hanjun Li</a>, <a href="https://scholar.google.com/citations?user=tkUBeeQAAAAJ&hl=en">Kun-Yu Lin</a>, <a href="https://scholar.google.com/citations?hl=en&user=bMedjfUAAAAJ">Junwei Liang</a>
    <br>
    <em>arxiv</em>, 2024
    <br>
    <a href="https://arxiv.org/abs/2311.17118">[paper]</a>
    <!-- <a href="https://jiaming-zhou.github.io/">supp</a> / -->
    <a href="data/AdaFocus_arxiv2023_bib.html">[bibtex]</a>
    <a href="https://jiaming-zhou.github.io/projects/AdaFocus/">[project page]</a>
    <p style="font-size: 16px; margin-top: 2px; margin-bottom: 20px;">The first weakly supervised framework for developing efficient end-to-end action recognition models on long videos, which gives birth to a new weakly supervised pipeline for downstream long-video tasks.</p>
  </td>
</tr>


<tr>
  <td style="padding:20px;width:25%;vertical-align:middle">
    <img src="images/GLOVER_arxiv2025.jpg" alt="PontTuset" width="180" style="border-style: none">
  </td>
  <td width="75%" valign="middle">
    <a href="https://arxiv.org/abs/2411.12286" id="GLOVER">
      <span class="papertitle">GLOVER: Generalizable Open-Vocabulary Affordance Reasoning
        for Task-Oriented Grasping</span>
    </a>
    <br>
    <a href="https://scholar.google.com/citations?user=arny77IAAAAJ&hl=en">Teli Ma</a>,  <a href="https://openreview.net/profile?id=~Zifan_Wang7"> Zifan Wang</a>, <strong>Jiaming Zhou</strong>, <a href="https://sallymmx.github.io/">Mengmeng Wang</a>, <a href="https://scholar.google.com/citations?hl=en&user=bMedjfUAAAAJ">Junwei Liang</a>
    <br>
    <em>arxiv</em>, 2025
    <br>
    <a href="https://arxiv.org/pdf/2411.12286">[paper]</a>
    <a href="https://teleema.github.io/projects/GLOVER/">[project page]</a>
    <a href="data/GLOVER_arxiv2025_bib.html">[bibtex]</a> 
    <p></p>
  </td>
</tr>

<tr>
  <td style="padding:20px;width:25%;vertical-align:middle">
    <img src="images/Contrastive_IL_arXiv2024.jpg" alt="PontTuset" width="180" style="border-style: none">
  </td>
  <td width="75%" valign="middle">
    <a href="https://arxiv.org/abs/2406.09738" id="Contrastive_IL">
      <span class="papertitle">Contrastive Imitation Learning for Language-guided Multi-Task Robotic Manipulation</span>
    </a>
    <br>
    <a href="https://scholar.google.com/citations?user=arny77IAAAAJ&hl=en">Teli Ma</a>, <strong>Jiaming Zhou</strong>,  <a href="https://openreview.net/profile?id=~Zifan_Wang7"> Zifan Wang</a>, <a href="https://scholar.google.com/citations?hl=en&user=QOwOHZMAAAAJ">Ronghe Qiu</a>, <a href="https://scholar.google.com/citations?hl=en&user=bMedjfUAAAAJ">Junwei Liang</a>
    <br>
    <em>CoRL</em>, 2024
    <br>
    <a href="https://arxiv.org/pdf/2406.09738">[paper]</a>
    <a href="https://teleema.github.io/projects/Sigma_Agent/">[project page]</a>
    <a href="data/Contrastive_IL_arxiv2024_bib.html">[bibtex]</a> 
    <p></p>
  </td>
</tr>



<tr>
  <td style="padding:20px;width:25%;vertical-align:middle">
    <img src="images/ActionHub_arXiv2024.jpg" alt="PontTuset" width="180" style="border-style: none">
  </td>
  <td width="75%" valign="middle">
    <a href="https://arxiv.org/pdf/2401.11654.pdf" id="ActionHub_arxiv2024">
      <span class="papertitle">ActionHub: A Large-scale Action Video Description Dataset for Zero-shot Action Recognition</span>
    </a>
    <br>
    <strong>Jiaming Zhou</strong>,  <a href="https://scholar.google.com/citations?hl=en&user=bMedjfUAAAAJ">Junwei Liang</a>, <a href="https://scholar.google.com/citations?user=tkUBeeQAAAAJ&hl=en">Kun-Yu Lin</a>, <a href="https://scholar.google.com/citations?user=8iBtvCQAAAAJ">Jinrui Yang</a>, <a href="https://scholar.google.com/citations?user=AwqDDGoAAAAJ&hl=zh-CN">Wei-Shi Zheng</a>
    <br>
    <em>arxiv</em>, 2024
    <br>
    <a href="https://arxiv.org/abs/2401.11654">[paper]</a>
    <!-- <a href="https://jiaming-zhou.github.io/">supp</a> / -->
    <a href="data/ActionHub_arxiv2024_bib.html">[bibtex]</a>
    <!-- <a href="https://jiaming-zhou.github.io/projects/AdaFocus/">[project page]</a> -->
    <p style="font-size: 16px; margin-top: 2px; margin-bottom: 20px;">A large-scale action video description dataset named ActionHub is proposed, which is the first, and the largest dataset that provides millions of video descriptions to describe thousands of human actions.</p>

  </td>
</tr>

<tr>
  <td style="padding:20px;width:25%;vertical-align:middle">
    <img src="images/TwinFormer_TMM2023.jpg" alt="PontTuset" width="180" style="border-style: none">
  </td>
  <td width="75%" valign="middle">
    <a href="https://ieeexplore.ieee.org/abstract/document/10210078" id="TwinFormer_TMM2023">
      <span class="papertitle">TwinFormer: Fine-to-Coarse Temporal Modeling for Long-term Action Recognition</span>
    </a>
    <br>
    <strong>Jiaming Zhou</strong>, <a href="https://scholar.google.com/citations?user=tkUBeeQAAAAJ&hl=en">Kun-Yu Lin</a>, <a href="https://openreview.net/profile?id=~Yu-Kun_Qiu1">Yu-Kun Qiu</a>, <a href="https://scholar.google.com/citations?user=AwqDDGoAAAAJ&hl=zh-CN">Wei-Shi Zheng</a>
    <br>
    <em>Transactions on Multimedia (TMM)</em>, 2023
    <br>
    <a href="https://ieeexplore.ieee.org/abstract/document/10210078">[paper]</a>
    <a href="data/TwinFormer_TMM2023_bib.html">[bibtex]</a> 
    <p></p>
  </td>
</tr>

<tr>
  <td style="padding:20px;width:25%;vertical-align:middle">
    <img src="images/GHRM_CVPR2021.jpg" alt="PontTuset" width="180" style="border-style: none">
  </td>
  <td width="75%" valign="middle">
    <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Zhou_Graph-Based_High-Order_Relation_Modeling_for_Long-Term_Action_Recognition_CVPR_2021_paper.pdf" id="GHRM_CVPR2021">
      <span class="papertitle">Graph-based High-order Relation Modeling for Long-term Action Recognition</span>
    </a>
    <br>
    <strong>Jiaming Zhou</strong>, <a href="https://scholar.google.com/citations?user=tkUBeeQAAAAJ&hl=en">Kun-Yu Lin</a>, <a href="https://scholar.google.com/citations?user=NTT88q4AAAAJ&hl=zh-CN">Haoxin Li</a>, <a href="https://scholar.google.com/citations?user=AwqDDGoAAAAJ&hl=zh-CN">Wei-Shi Zheng</a>
    <br>
    <em>CVPR</em>, 2021
    <br>
    <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Zhou_Graph-Based_High-Order_Relation_Modeling_for_Long-Term_Action_Recognition_CVPR_2021_paper.pdf">[paper]</a>
    <a href="https://openaccess.thecvf.com/content/CVPR2021/supplemental/Zhou_Graph-Based_High-Order_Relation_CVPR_2021_supplemental.pdf">[supp]</a>
    <a href="data/GHRM_CVPR2021_bib.html">[bibtex]</a> 
    <p></p>
    <!-- <p>We produce state-of-the-art contours, regions and object candidates, and we compute normalized-cuts eigenvectors 20&times faster.</p> -->
    <!-- <p>This paper subsumes our CVPR 2014 paper.</p> -->
  </td>
</tr>


<tr>
  <td style="padding:20px;width:25%;vertical-align:middle">
    <img src="images/XOV_Action_arXiv2024.jpg" alt="PontTuset" width="180" style="border-style: none">
  </td>
  <td width="75%" valign="middle">
    <a href="https://arxiv.org/pdf/2403.01560.pdf" id="XOV_Action_arxiv2024">
      <span class="papertitle">Rethinking CLIP-based Video Learners in Cross-Domain Open-Vocabulary Action Recognition</span>
    </a>
    <br>
    <a href="https://scholar.google.com/citations?user=tkUBeeQAAAAJ&hl=en">Kun-Yu Lin</a>, <a href="https://scholar.google.com/citations?user=WI_flSwAAAAJ&hl=en">Henghui Ding</a>, <strong>Jiaming Zhou</strong>, <a href="https://openreview.net/profile?id=~Yi-Xing_Peng1">Yi-Xing Peng</a>, <a href="https://scholar.google.com/citations?user=3e8zto0AAAAJ&hl=en">Zhilin Zhao</a>, <a href="https://scholar.google.com/citations?user=559LF80AAAAJ&hl=en">Chen Change Loy</a>, <a href="https://scholar.google.com/citations?user=AwqDDGoAAAAJ&hl=zh-CN">Wei-Shi Zheng</a>
    <br>
    <em>arxiv</em>, 2024
    <br>
    <a href="https://arxiv.org/abs/2403.01560">[paper]</a>
    <!-- <a href="https://jiaming-zhou.github.io/">supp</a> / -->
    <a href="data/XOV_Action_arxiv2024_bib.html">[bibtex]</a>
    <a href="https://github.com/KunyuLin/XOV-Action/">[github]</a>
    <p style="font-size: 16px; margin-top: 2px; margin-bottom: 20px;">The first benchmark, named XOV-Action, for the cross-domain open-vocabulary action recognition task, and a simple yet effective method to address the scene bias for the task.</p>
  </td>
</tr>



<tr>
  <td style="padding:20px;width:25%;vertical-align:middle">
    <img src="images/HCTransformer_TPAMI.png" alt="PontTuset" width="180" style="border-style: none">
  </td>
  <td width="75%" valign="middle">
    <a href="https://arxiv.org/abs/2407.10860" id="HCTransformer">
      <span class="papertitle">Human-Centric Transformer for Domain Adaptive Action Recognition</span>
    </a>
    <br>
    <a href="https://scholar.google.com/citations?user=tkUBeeQAAAAJ&hl=en">Kun-Yu Lin</a>, <strong>Jiaming Zhou</strong>, <a href="https://scholar.google.com/citations?user=AwqDDGoAAAAJ&hl=zh-CN">Wei-Shi Zheng</a>
    <br>
    <em>TPAMI</em>, 2024
    <br>
    <a href="https://arxiv.org/pdf/2407.10860">[paper]</a>
    <!--<a href="data/Contrastive_IL_arxiv2024_bib.html">[bibtex]</a>-->
    <p></p>
  </td>
</tr>


<tr>
  <td style="padding:20px;width:25%;vertical-align:middle">
    <img src="images/GeoDeformer_arXiv2023.jpg" alt="PontTuset" width="180" style="border-style: none">
  </td>
  <td width="75%" valign="middle">
    <a href="https://arxiv.org/pdf/2311.17975.pdf" id="GeoDeformer_arXiv2023">
      <span class="papertitle">GeoDeformer: Geometric Deformable Transformer for Action Recognition</span>
    </a>
    <br>
    <a href="https://scholar.google.com/citations?user=RpXhJu0AAAAJ&hl=zh-CN&oi=ao">Jinhui Ye</a>, <strong>Jiaming Zhou</strong>, <a href="https://scholar.google.com/citations?user=cVDF1tkAAAAJ&hl=en">Hui Xiong</a>, <a href="https://scholar.google.com/citations?hl=en&user=bMedjfUAAAAJ">Junwei Liang</a>
    <br>
    <em>arxiv</em>, 2023
    <br>
    <a href="https://arxiv.org/pdf/2311.17975.pdf">[paper]</a>
    <a href="data/GeoDeformer_arxiv2023_bib.html">[bibtex]</a> 
    <p></p>
  </td>
</tr>

<tr>
  <td style="padding:20px;width:25%;vertical-align:middle">
    <img src="images/STDN_NeurIPS2023.jpg" alt="PontTuset" width="180" style="border-style: none">
  </td>
  <td width="75%" valign="middle">
    <a href="https://openreview.net/forum?id=YsZTDcIQwQ" id="STDN_NeurIPS2023">
      <span class="papertitle">Diversifying Spatial-Temporal Perception for Video Domain Generalization</span>
    </a>
    <br>
    <a href="https://scholar.google.com/citations?user=tkUBeeQAAAAJ&hl=en">Kun-Yu Lin</a>, <a href="https://openreview.net/profile?id=~Jia-Run_Du1">Jia-Run Du</a>, <a href="https://scholar.google.com/citations?user=m8I1ELMAAAAJ">Yipeng Gao</a>, <strong>Jiaming Zhou</strong>, <a href="https://scholar.google.com/citations?user=AwqDDGoAAAAJ&hl=zh-CN">Wei-Shi Zheng</a>
    <br>
    <em>NeurIPS</em>, 2023
    <br>
    <a href="https://openreview.net/pdf?id=YsZTDcIQwQ">[paper]</a>
    <a href="https://openreview.net/attachment?id=YsZTDcIQwQ&name=supplementary_material">[supp]</a>
    <a href="data/STDN_NeurIPS2023_bib.html">[bibtex]</a> 
    <a href="https://github.com/KunyuLin/STDN/">[github]</a>
    <p></p>
  </td>
</tr>



<tr>
  <td style="padding:20px;width:25%;vertical-align:middle">
    <img src="images/Cycle_ECCV2022.jpg" alt="PontTuset" width="180" style="border-style: none">
  </td>
  <td width="75%" valign="middle">
    <a href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136930520.pdf" id="Cycle_ECCV2022">
      <span class="papertitle">Adversarial Partial Domain Adaptation by Cycle Inconsistency</span>
    </a>
    <br>
    <a href="https://scholar.google.com/citations?user=tkUBeeQAAAAJ&hl=en">Kun-Yu Lin</a>, <strong>Jiaming Zhou</strong> (co-first), <a href="https://openreview.net/profile?id=~Yu-Kun_Qiu1">Yu-Kun Qiu</a>, <a href="https://scholar.google.com/citations?user=AwqDDGoAAAAJ&hl=zh-CN">Wei-Shi Zheng</a>
    <br>
    <em>ECCV</em>, 2022
    <br>
    <a href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136930520.pdf">[paper]</a>
    <a href="https://static-content.springer.com/esm/chp%3A10.1007%2F978-3-031-19827-4_31/MediaObjects/540002_1_En_31_MOESM1_ESM.pdf">[supp]</a>
    <a href="data/Cycle_inconsistency_ECCV2022_bib.html">[bibtex]</a> 
    <p></p>
  </td>
</tr>



<tr>  <td> &nbsp; </td></tr>



</tbody></table>
<table style="width:110%;border:0px;border-spacing:5px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
  <tr><h2>Services</h2></tr>
  <tr> <td>&nbsp; &nbsp; &nbsp; &nbsp; Reviewers of <b>CVPR</b> 2022-Now, <b>ECCV</b> 2022-Now, <b>ICCV</b> 2023-Now. </td></tr>
  <tr> <td>&nbsp; &nbsp; &nbsp; &nbsp; Reviewers of <b>NeurIPS</b> 2024-Now, <b>ICML</b> 2024-Now, <b>ICLR</b> 2024-Now. </td></tr>
  <tr> <td>&nbsp; &nbsp; &nbsp; &nbsp; Reviewer of <b>Transactions on Multimedia</b>.</td></tr>
  <tr> <td>&nbsp; &nbsp; &nbsp; &nbsp; Reviewer of <b>Pattern Recognition</b>.</td></tr>
  <tr>  <td> &nbsp; </td></tr>


<!--
</tbody></table>
<table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
  <tr>
    <td>
      <h2>Miscellanea</h2>
    </td>
  </tr>
</tbody></table>
<table width="100%" align="center" border="0" cellpadding="20"><tbody>
  
  <tr>
    <td style="padding:20px;width:25%;vertical-align:middle"><img src="images/cvf.jpg"></td>
    <td width="75%" valign="center">
      <a href="https://cvpr2023.thecvf.com/Conferences/2023/Organizers">Demo Chair, CVPR 2023</a>
<br>
      <a href="https://cvpr2022.thecvf.com/area-chairs">Area Chair, CVPR 2022</a>
      <br>
      <a href="http://cvpr2021.thecvf.com/area-chairs">Area Chair & Longuet-Higgins Award Committee Member, CVPR 2021</a>
      <br>
      <a href="http://cvpr2019.thecvf.com/area_chairs">Area Chair, CVPR 2019</a>
      <br>
      <a href="http://cvpr2018.thecvf.com/organizers/area_chairs">Area Chair, CVPR 2018</a>
    </td>
  </tr>
  <tr>
    <td style="padding:20px;width:25%;vertical-align:middle">
      <img src="images/cs188.jpg" alt="cs188">
    </td>
    <td width="75%" valign="center">
      <a href="http://inst.eecs.berkeley.edu/~cs188/sp11/announcements.html">Graduate Student Instructor, CS188 Spring 2011</a>
      <br>
      <a href="http://inst.eecs.berkeley.edu/~cs188/fa10/announcements.html">Graduate Student Instructor, CS188 Fall 2010</a>
      <br>
      <a href="http://aima.cs.berkeley.edu/">Figures, "Artificial Intelligence: A Modern Approach", 3rd Edition</a>
    </td>
  </tr>

  <tr>
    <td align="center" style="padding:20px;width:25%;vertical-align:middle">
      <h2>Basically <br> Blog Posts</h2>
    </td>
    <td width="75%" valign="middle">
      <a href="https://arxiv.org/abs/2112.11687">Squareplus: A Softplus-Like Algebraic Rectifier</a>
      <br>
      <a href="https://arxiv.org/abs/2010.09714">A Convenient Generalization of Schlick's Bias and Gain Functions</a>
      <br>
      <a href="https://arxiv.org/abs/1704.07483">Continuously Differentiable Exponential Linear Units</a>
      <br>
      <a href="https://jonbarron.info/data/cvpr2023_llm_workshop_annotated.pdf">Scholars & Big Models: How Can Academics Adapt?</a>
    </td>
  </tr>
  
  
</tbody></table>
-->

<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
  <tr>
    <td style="padding:0px">
      <br>
      <p style="text-align:center;font-size:small;">
        This website borrows from <a href="https://github.com/jonbarron/jonbarron_website">Jon Barron</a>.
      </p>
    </td>
  </tr>
</tbody></table>
        
        </td>
      </tr>
    </table>
  </body>
</html>
