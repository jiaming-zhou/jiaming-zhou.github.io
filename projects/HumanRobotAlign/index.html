<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Mitigating the Human-Robot Domain Discrepancy in Visual Pre-training for Robotic Manipulation</title>
  <link rel="icon" type="image/x-icon" href="static/images/UST.svg.png">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Mitigating the Human-Robot Domain Discrepancy in Visual Pre-training for Robotic Manipulation</h1>
            <h2 class="is-size-5">CVPR 2025</h2>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <!-- <a href="FIRST AUTHOR PERSONAL LINK" target="_blank">Jiaming Zhou</a><sup>*</sup>,</span> -->
                <a href="https://jiaming-zhou.github.io/" target="_blank">Jiaming Zhou</a><sup>1</sup> &nbsp;</span>
                <span class="author-block">
                <a href="https://scholar.google.com/citations?user=arny77IAAAAJ&hl=en" target="_blank">Teli Ma</a><sup>1</sup> &nbsp;</span>
                <span class="author-block">
                <a href="https://scholar.google.com/citations?user=tkUBeeQAAAAJ&hl=en" target="_blank">Kun-Yu Lin</a><sup>2</sup> &nbsp;</span>
                <span class="author-block">
                  <a href="https://scholar.google.com/citations?hl=en&user=QOwOHZMAAAAJ" target="_blank">Ronghe Qiu</a><sup>1</sup> &nbsp;</span>
                  <span class="author-block">
                    <a href="https://openreview.net/profile?id=~Zifan_Wang7" target="_blank">Zifan Wang</a><sup>1</sup> &nbsp;</span>
                <span class="author-block">
                <a href="https://junweiliang.me" target="_blank">Junwei Liang</a><sup>1,3,&dagger;</sup>
                </span>
                </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block"><sup>1</sup>AI Thrust, The Hong Kong University of Science and Technology (Guangzhou)</span>
                    <span class="author-block"><sup>2</sup>Computer Science and Engineering, Sun Yat-sen University</span>
                    <span class="author-block"><sup>3</sup>CSE, The Hong Kong University of Science and Technology</span>
                    <!-- <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span> -->
                    <span class="eql-cntrb"><small><br>&dagger; Indicates Corresponding Author</small></span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                      <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/abs/2406.14235" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <!-- Supplementary PDF link -->
                    <!-- <span class="link-block">
                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span> -->

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/jiaming-zhou/HumanRobotAlign" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2406.14235" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              </a>
            </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<div class="container is-max-desktop">
  <h6 class="title is-5" align="center" style="color: #000000">The code, data, and models will be released upon the acceptance of paper.</h6>
</div>


<!-- Teaser video-->
<!-- <section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video poster="" id="tree" autoplay controls muted loop height="100%">
        <source src="static/videos/banner_video.mp4"
        type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        Aliquam vitae elit ullamcorper tellus egestas pellentesque. Ut lacus tellus, maximus vel lectus at, placerat pretium mi. Maecenas dignissim tincidunt vestibulum. Sed consequat hendrerit nisl ut maximus. 
      </h2>
    </div>
  </div>
</section> -->
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Learning generalizable visual dynamic representation across different embodied environments is crucial for real-world robotic manipulation. 
            As the scale and diversity of robot demonstration data are limited, recent works have turned to large-scale pre-training using human data.
            However, the morphological differences between humans and robots introduce a significant human-robot domain discrepancy, challenging the generalization of these human-data pre-trained models to downstream manipulation tasks.
            To address this, we propose a novel adaptation paradigm that utilizes readily available paired human-robot video data to bridge the discrepancy. 
            Following this paradigm, our method exploits a human-robot contrastive alignment loss to align the semantics of human and robot videos, adapting pre-trained models to the robotic domain in a parameter-efficient manner.
            The experiments demonstrate significant improvements on 25 tasks across three different benchmarks, where the single-task, language-conditioned multi-task settings are covered, and two different pre-trained models are evaluated. 
            On the large RLBench benchmark, our adaptation method achieves an average improvement of 8.9% in success rate over the pre-trained R3M model across multiple tasks.  
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<section class="section is-small">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full">
        <div class="content">
          <h2 class="title is-4">Contribution #1</h2>
          <div class="level-set has-text-justified">
            <p>We highlight the human-robot domain discrepancy in visual pre-training for robotic manipulation, and provide a new adaptation paradigm that simultaneously alleviates the domain discrepancy and maintains the versatility of pre-trained models; </p>
            <p></p>
            </div>
          </div>
          <img src="static/images/motivation.jpg" alt="motivation" style="margin-left: 60px;" width="800" height="200">
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section is-small">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full">
        <div class="content">
          <h2 class="title is-4">Contribution #2</h2>
          <div class="level-set has-text-justified">
            <p>We propose a Human-Robot Semantic Alignment method, which adapts pre-trained models with parameter-efficient design and exploits a human-robot contrastive alignment loss for effectively mitigating the domain discrepancy;</p>
            <p></p>
            </div>
          </div>
          <img src="static/images/method.jpg" alt="method" style="margin-left: 60px;" width="800" height="200">
        </div>
      </div>
    </div>
  </div>
</section>


<section class="is-small">
  <!-- <div class="hero-body"> -->
    <div class="container is-max-desktop">
      <h2 class="title is-4" style="color: #000000">Contribution #3</h2>
      <p style="color: #030303">We evaluate the effectiveness of our method in three different environments, covering 7 single-tasks and 18 language-conditioned multi-tasks, as well as pre-trained models with different pre-training methodologies.</p>
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item blend-img-item item-full-pro">
          <img src="static/images/downstream_adroit.jpg" width="600" height="600" style="margin-left: 150px;" alt="Detailed PRO" class="center-image blend-img-background"/>
          <h2 class="subtitle has-text-centered">
            Manipulation Success Rate of two tasks in Adroit.
          </h2>
        </div>
        <div class="item blend-img-item item-full-iroc">
          <img src="static/images/downstream_metaworld.jpg" width="800" height="400" style="margin-left: 50px;" alt="Detailed Image ROCAUC" class="center-image blend-img-background"/>
          <h2 class="subtitle has-text-centered">
            Manipulation Success Rate of five tasks in Metaworld.
          </h2>
        </div>
        <div class="item blend-img-item item-full-iroc">
          <img src="static/images/downstream_rlbench.jpg" width="800" height="600" alt="Detailed Image ROCAUC" class="center-image blend-img-background"/>
          <h2 class="subtitle has-text-centered">
            Manipulation Success Rate of 18 tasks in RLBench.
          </h2>
        </div>
        </div>
      </div>
    </div>
  <!-- </div> -->
</section>


<!-- Paper Conclusion -->
<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Conclusion</h2>
        <div class="content has-text-justified">
          <p>
            The existing learning paradigm of visual pre-training on human data for robotic manipulation encounters the human-robot domain discrepancy. 
            Our work takes a preliminary attempt to solve this challenging problem.
            In this work, we contribute a new adaptation paradigm by leveraging existing semantic-aligned human-robot video data and proposing an efficient semantic alignment method. 
            In this way, the existing human-data pre-trained models can be efficiently and explicitly adapted to the robot domain, without the need to be tailored for each downstream robotic environment.
            Experiments on 25 robotic manipulation tasks across three environments and different pre-trained models demonstrate the efficacy of our proposed method.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper Conclusion -->



<!-- Image carousel -->
<!-- <section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item">
        <img src="static/images/carousel1.jpg" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          First image description.
        </h2>
      </div>
      <div class="item">
        <img src="static/images/carousel2.jpg" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          Second image description.
        </h2>
      </div>
  </div>
</div>
</div>
</section> -->
<!-- End image carousel -->




<!-- Youtube video -->
<!-- <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Video Presentation</h2>
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          
          <div class="publication-video">
            <iframe src="https://www.youtube.com/embed/JkaxUblCGz0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
          </div>
        </div>
      </div>
    </div>
  </div>
</section> -->
<!-- End youtube video -->


<!-- Video carousel -->
<!-- <section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Another Carousel</h2>
      <div id="results-carousel" class="carousel results-carousel">
        <div class="item item-video1">
          <video poster="" id="video1" autoplay controls muted loop height="100%">
            <source src="static/videos/carousel1.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video2">
          <video poster="" id="video2" autoplay controls muted loop height="100%">
            <source src="static/videos/carousel2.mp4"
            type="video/mp4">
          </video>
        </div>
        <div class="item item-video3">
          <video poster="" id="video3" autoplay controls muted loop height="100%">\
            <source src="static/videos/carousel3.mp4"
            type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </div>
</section> -->
<!-- End video carousel -->






<!-- Paper poster -->
<!-- <section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Poster</h2>

      <iframe  src="static/pdfs/sample.pdf" width="100%" height="550">
          </iframe>
        
      </div>
    </div>
  </section> -->
<!--End paper poster -->


<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@article{zhou2024mitigating,
        title={Mitigating the Human-Robot Domain Discrepancy in Visual Pre-training for Robotic Manipulation},
        author={Zhou, Jiaming and Ma, Teli and Lin, Kun-Yu and Qiu, Ronghe and Wang, Zifan and Liang, Junwei},
        journal={arXiv preprint arXiv:2406.14235},
        year={2024}
      }
      </code></pre>
    </div>
</section>
<!--End BibTex citation -->

  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p style="text-align:center;font-size:small;">
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
